{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oYxEVNTKusl"
      },
      "source": [
        "This codebook will demonstrate working on the Prophet algorithm using the Adobe historical stock dataset. First, the necessary libraries need to be imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ypq5jEIq67N-"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import numpy as np #for powerful array and fast math operations\n",
        "import pandas as pd #for creating and working on dataframes\n",
        "import matplotlib.pyplot as plt #for creating plots and charts\n",
        "import seaborn as sns #another visualization library for better and easier plots\n",
        "import kagglehub #for interacting with kaggle to download datasets\n",
        "import os #for interacting with the os to read csv\n",
        "import prophet #importing the prophet library\n",
        "from prophet.make_holidays import make_holidays_df #importing holidays for the algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the dataset needs to be loaded. It will be loaded and handled with kagglehub and os libraries."
      ],
      "metadata": {
        "id": "k7m8X7uP8dxf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0Jhe_Cx6eFA"
      },
      "outputs": [],
      "source": [
        "# Download latest version of the dataset\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/stock-market-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypRUUy9S8FCN"
      },
      "outputs": [],
      "source": [
        "# `path` points to the downloaded dataset directory\n",
        "dataset_dir = path\n",
        "\n",
        "# List files in the dataset directory to identify the data files\n",
        "print(\"Files in dataset directory:\", os.listdir(dataset_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the dataset will be read using the read.csv function and the first few columns along with data types will be observed."
      ],
      "metadata": {
        "id": "sqIMPonP86yG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeBfEZql8Fes"
      },
      "outputs": [],
      "source": [
        "# Load Adobe stock data\n",
        "adbe_path = os.path.join(dataset_dir, \"stock_market_data\", \"nasdaq\", \"csv\", \"ADBE.csv\")\n",
        "adbe_df = pd.read_csv(adbe_path)\n",
        "\n",
        "# Inspect first rows\n",
        "print(adbe_df.head())\n",
        "print(adbe_df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwiQr6NoLych"
      },
      "source": [
        "The output from the last cell showed that the 'Date' column contains object datatype. But Prophet requires the the date to be datetime format. The following cell will convert the dates to datetime format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZPygw3T8bsZ"
      },
      "outputs": [],
      "source": [
        "# Turning dates into datetime format\n",
        "adbe_df['Date'] = pd.to_datetime(adbe_df['Date'], dayfirst = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCIWzAuE88s1"
      },
      "outputs": [],
      "source": [
        "adbe_df.tail(n=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKDfwassMg1Y"
      },
      "source": [
        "Now the dataset needs to be checked for any missing dates. For this, I'll first find out the total range of date the data spans over. From this, I'll calculate the total calendar days. Then I'll subtract from it the available dates in my dataset, the holidays, and the weekends. This will give me the missing dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjfWJvKXNXd_"
      },
      "outputs": [],
      "source": [
        "# Find the total number of dates available in the dataset\n",
        "start_date = adbe_df['Date'].min()\n",
        "end_date = adbe_df['Date'].max()\n",
        "\n",
        "print(f\"Date range: {start_date} to {end_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiVkntpp-7o6"
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of calendar days in the timeline\n",
        "full_timeline = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "total_calendar_days = len(full_timeline)\n",
        "# Calculate the number of days available in the dataset\n",
        "available_data_days = len(adbe_df)\n",
        "print(f\"Total calendar days in the timeline: {total_calendar_days}\")\n",
        "print(f\"Number of days with available data: {available_data_days}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNdABAuq_9RP"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of weekends in the timeline\n",
        "number_of_weekends = sum(full_timeline.dayofweek >= 5)\n",
        "print(f\"Number of weekend days (Sat/Sun): {number_of_weekends}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnHX6x8kAq04"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of holidays for the period\n",
        "year_list = range(start_date.year, end_date.year + 1)\n",
        "holidays_df = make_holidays_df(year_list=year_list, country='US')\n",
        "# Filter holidays to be within the timeline and not on a weekend\n",
        "holidays_in_timeline = holidays_df[(holidays_df['ds'] >= start_date) & (holidays_df['ds'] <= end_date) & (holidays_df['ds'].dt.dayofweek < 5)]\n",
        "number_of_holidays = len(holidays_in_timeline)\n",
        "print(f\"Number of public holiday weekdays: {number_of_holidays}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60dxaWsGCOkd"
      },
      "outputs": [],
      "source": [
        "# find the days that are missing on the dataset\n",
        "missing_days = total_calendar_days - available_data_days - number_of_weekends - number_of_holidays\n",
        "print(f\"Missing days in the dataset: {missing_days}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EykJTVQ1OWG-"
      },
      "source": [
        "Here the negative number is because prophet's holiday library includes some holidays on which stock market stays open. Since the number is only 45 in the span of 36 years, this is minimal and therefore it is ignoreable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yl8s0FYOm8X"
      },
      "source": [
        "Now I'll prepare my data for Prophet algorithm. Since 2020 contains the COVID pandemic, an anomaly that caused a lot of volatility, I'll train and test my model with data up till 2020."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeqV3VpW9EfD"
      },
      "outputs": [],
      "source": [
        "# Filter a range for model training\n",
        "start_date = '1986-08-13'\n",
        "end_date = '2019-12-31'\n",
        "\n",
        "# Filter ADBE stock\n",
        "adbe_train = adbe_df[(adbe_df['Date'] >= start_date) & (adbe_df['Date'] <= end_date)].copy()\n",
        "\n",
        "adbe_train = adbe_train.set_index('Date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to0ydU6t_LdK"
      },
      "outputs": [],
      "source": [
        "# inspect the dataset\n",
        "adbe_train.head(n=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wbkvk45PQLy"
      },
      "source": [
        "Now I'll check for any duplicates dates, because each date should only have one value for each of the columns and therefore only one row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJljcNrlAr3C"
      },
      "outputs": [],
      "source": [
        "# check for duplictaes\n",
        "duplicate = adbe_train.reset_index().duplicated(subset=['Date'])\n",
        "adbe_train[duplicate.values]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsjLc-UkPkCl"
      },
      "source": [
        "Since there are no duplicates found, I'll go forward to check the relationship between the variables. The 'pairplot' function shows the realationship between each variable in separate graphs, making it easy to inspect the realtion and find an anomaly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V-5a3phCh24"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(adbe_train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqSgVv98nsnh"
      },
      "source": [
        "Now I'll create a new dataframe with only the necessary columns. Prophet requires the 'Dates' column to be named 'ds', and the column we'll predict (here 'Adjusted Close') to be named 'y'. Tlog transform the variables, because it stabilizes the variables and handles non-linear growth, that are usually seen in stock data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNwBzZH-yFX0"
      },
      "outputs": [],
      "source": [
        "train_df = adbe_train[['Adjusted Close']].reset_index()\n",
        "train_df.rename(columns={'Date': 'ds', 'Adjusted Close': 'y'}, inplace=True)\n",
        "train_df.head(n=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvovnpHUDcJK"
      },
      "outputs": [],
      "source": [
        "# log transformation\n",
        "import numpy as np\n",
        "train_df_log = train_df.copy()\n",
        "train_df_log['y'] = np.log1p(train_df_log['y'])\n",
        "train_df_log.head(n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll train 2 models. At first will be using the built in methods without any added regressors. Then I'll add some regressors to see if that helps the model's predictions."
      ],
      "metadata": {
        "id": "QhrUxiye_mQk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC6Gqg0a2YLH"
      },
      "source": [
        "I'll use the same train size, horizon, and storage for metrices for comparability. i'll train on 90% of data using a 60 day horizon for forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bizaXLZf1iCt"
      },
      "outputs": [],
      "source": [
        "# Parameters for model traininng\n",
        "initial_train_size = int(0.9 * len(train_df_log))\n",
        "horizon = 60\n",
        "n_iterations = (len(train_df_log) - initial_train_size) // horizon\n",
        "\n",
        "# Storage for metrics\n",
        "metrics_1 = {'mae': [], 'rmse': [], 'mape': [], 'r2': []}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll import holidays from the Prophet's built in methods. Adding holidays is importatnt because holidays introduce sudden spikes in stock proces."
      ],
      "metadata": {
        "id": "IkbcoTYJAVCP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyJAQV_PP1-a"
      },
      "outputs": [],
      "source": [
        "from prophet import Prophet\n",
        "from prophet.make_holidays import make_holidays_df\n",
        "\n",
        "# Make holidays DataFrame for US\n",
        "holidays = make_holidays_df(year_list=range(1986, 2022), country='US')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll train the first model. It'll be a basic model without regressors using the Prophet's built in cross validation function for this."
      ],
      "metadata": {
        "id": "HTE9kBSdAipk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UD4gx65fTUI"
      },
      "outputs": [],
      "source": [
        "# built in cross validation model (without any added regressors)\n",
        "import prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Suppress warnings using logging\n",
        "logging.getLogger('prophet').setLevel(logging.ERROR)\n",
        "\n",
        "# Initialize and tune Prophet with built-in components\n",
        "model_1 = Prophet(\n",
        "    growth='linear', #assuming data grows and shrinks at steady rate without caps\n",
        "    daily_seasonality=False, #since there's no hourly data\n",
        "    weekly_seasonality=True,\n",
        "    yearly_seasonality=True,\n",
        "    seasonality_mode='additive', #because the data is log-transformed\n",
        "    changepoint_prior_scale=0.5, #higher value for more flexibility and trend changes\n",
        "    interval_width=0.95, #95% confidence interval\n",
        "    uncertainty_samples=1000, #more samples for more stable interval\n",
        "    seasonality_prior_scale=0.1, #small to smooth out the seasonality to reduce overfitting\n",
        "    holidays=holidays\n",
        ")\n",
        "\n",
        "# Add quarterly seasonality to reflect quaterly patterns\n",
        "model_1.add_seasonality(name='quarterly', period=91.3, fourier_order=5) #stock data usually show quarterly seasonality\n",
        "\n",
        "print(f\"\\nTraining Prophet on {len(train_df_log)} data points\")\n",
        "model_1.fit(train_df_log)\n",
        "\n",
        "# Cross-validation with parameter tuning\n",
        "print(f\"\\nRunning cross-validation:\")\n",
        "print(f\"  Initial training size: {initial_train_size} days (90% of data)\")\n",
        "print(f\"  Horizon: {horizon} days\")\n",
        "print(f\"  Number of folds: {n_iterations}\")\n",
        "\n",
        "df_cv = cross_validation(\n",
        "    model_1,\n",
        "    initial=f'{initial_train_size} days',\n",
        "    horizon=f'{horizon} days',\n",
        "    period=f'{horizon} days',\n",
        "    parallel='processes'\n",
        ")\n",
        "\n",
        "print(f\"  Total predictions made: {len(df_cv)}\")\n",
        "\n",
        "# Reverse log transformation for actual scale metrics\n",
        "df_cv['y_actual'] = np.expm1(df_cv['y'])\n",
        "df_cv['yhat_actual'] = np.expm1(df_cv['yhat'])\n",
        "df_cv['yhat_lower_actual'] = np.expm1(df_cv['yhat_lower'])\n",
        "df_cv['yhat_upper_actual'] = np.expm1(df_cv['yhat_upper'])\n",
        "\n",
        "# Store predictions dataframe\n",
        "predictions_cv_no_regressors = df_cv.copy()\n",
        "\n",
        "# Calculate metrics in original scale\n",
        "mae = mean_absolute_error(df_cv['y_actual'], df_cv['yhat_actual'])\n",
        "rmse = np.sqrt(mean_squared_error(df_cv['y_actual'], df_cv['yhat_actual']))\n",
        "mape = np.mean(np.abs((df_cv['y_actual'] - df_cv['yhat_actual']) / (df_cv['y_actual'] + 1e-8))) * 100\n",
        "r2 = r2_score(df_cv['y_actual'], df_cv['yhat_actual'])\n",
        "\n",
        "# Coverage analysis\n",
        "in_interval = np.sum((df_cv['y_actual'] >= df_cv['yhat_lower_actual']) &\n",
        "                     (df_cv['y_actual'] <= df_cv['yhat_upper_actual']))\n",
        "coverage_pct = (in_interval / len(df_cv)) * 100\n",
        "avg_width_pct = np.mean((df_cv['yhat_upper_actual'] - df_cv['yhat_lower_actual']) /\n",
        "                        df_cv['y_actual']) * 100\n",
        "\n",
        "print(\"Model results with built_in components:\")\n",
        "\n",
        "print(f\"\\nMAE      = {mae:.3f}\")\n",
        "print(f\"RMSE     = {rmse:.3f}\")\n",
        "print(f\"MAPE     = {mape:.3f}\")\n",
        "print(f\"R²       = {r2:.4f}\")\n",
        "\n",
        "print(f\"\\nCOVERAGE = {coverage_pct:.1f}%\")\n",
        "print(f\"AVG WIDTH = {avg_width_pct:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although this model resulted in decent r2 value and coverage, the MAE, RMSE, and MAPE values are not ideal for a stock forecasting model."
      ],
      "metadata": {
        "id": "D0gxLKJzBgR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll use the following cell to save the predictions now, so I don't have to run it everytime my runtime disconnects to get the results."
      ],
      "metadata": {
        "id": "KaahpLt7AvEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the predictions after running model 1\n",
        "import pickle\n",
        "predictions_cv_no_regressors.to_csv('predictions_cv_no_regressors.csv', index=False)\n",
        "print(\"Saved predictions_cv_no_regressors\")"
      ],
      "metadata": {
        "id": "EV_gkNCAl4SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the predictions to chcek\n",
        "predictions_cv_no_regressors = pd.read_csv('predictions_no_regressors.csv', parse_dates=['ds'])\n",
        "predictions_cv_no_regressors.head()"
      ],
      "metadata": {
        "id": "v6jZbl8BWvLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll visualize all the components (weekly, yearly, and quately seasonality,holiday, and trend) using Prophet's built in plot_components. This gives us a clear idea about the patterns."
      ],
      "metadata": {
        "id": "EC80PJIMByuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize all the components of the dataset\n",
        "forecast_model_1 = model_1.predict(train_df_log)\n",
        "\n",
        "figure_1 = model_1.plot_components(forecast_model_1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GY1t1vgRzjKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll visualize the MAPE metric. It shows 4% instead of 16% because it's calculated in the log transformed space."
      ],
      "metadata": {
        "id": "51d57pZnCNJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet.plot import plot_cross_validation_metric\n",
        "# You can change 'mape' to 'rmse', 'mae', or 'mdape'.\n",
        "fiure_2 = plot_cross_validation_metric(df_cv, metric='mape')\n",
        "plt.title(\"Cross-Validation: MAPE vs. Horizon\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CayX7NI915DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-6BWxjADnQp"
      },
      "source": [
        "Now I'll train the model creating and adding some regressors to see if they improve the model's prediction. The regressors are not exogenous in nature, and are derived from this dataset. I'll use autoregression to predict future regressors and avoid any data leakage. The train size and horizon will be the same as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRbEN3sYRNDL"
      },
      "outputs": [],
      "source": [
        "# Parameters for iterative traininng\n",
        "initial_train_size = int(0.9 * len(train_df_log))\n",
        "horizon = 60\n",
        "n_iterations = (len(train_df_log) - initial_train_size) // horizon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiSFwEzbVxT9"
      },
      "outputs": [],
      "source": [
        "# Storage for metrics\n",
        "metrics = {'mae': [], 'rmse': [], 'mape': [], 'r2': []}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following model includes the following regressors: lag 1, 2, 3, and 7 (previous days' closing prices), simple moving average 7, and 30 (using rolling window means), 5 and 10 days' momentum, volatility 7 and 30 (using rolling window standard deviations), and trend strength (deviation from sma 30)."
      ],
      "metadata": {
        "id": "g7epXPeoDLLQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzT_Rwxa3K4L"
      },
      "outputs": [],
      "source": [
        "# Walk forward validation and performance metrics with autoregression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Main Walk-Forward Loop\n",
        "train_end = initial_train_size\n",
        "all_fold_forecasts = []\n",
        "coverage_metrics = {'in_coverage': [], 'coverage_width': []}\n",
        "\n",
        "# Store all predictions for later visualization\n",
        "all_predictions_list = []\n",
        "\n",
        "for i in range(n_iterations):\n",
        "    # Define current training and testing windows\n",
        "    current_train = train_df_log.iloc[:train_end].copy()\n",
        "    test_start = train_end\n",
        "    test_end = min(train_end + horizon, len(train_df_log))\n",
        "    test_df = train_df_log.iloc[test_start:test_end].copy()\n",
        "\n",
        "    if len(test_df) == 0:\n",
        "        break\n",
        "\n",
        "    print(f\"Iteration {i+1}/{n_iterations}: Training on {len(current_train)} data points, testing on {len(test_df)}.\")\n",
        "\n",
        "    # Recalculate features on the loop to avoid leakage\n",
        "    current_train_clean = current_train[['ds', 'y']].copy()\n",
        "\n",
        "    # Include basic lags\n",
        "    current_train_clean['lag1'] = current_train_clean['y'].shift(1)\n",
        "    current_train_clean['lag2'] = current_train_clean['y'].shift(2)\n",
        "    current_train_clean['lag3'] = current_train_clean['y'].shift(3)\n",
        "    current_train_clean['lag7'] = current_train_clean['y'].shift(7)\n",
        "\n",
        "    # Include moving averages\n",
        "    current_train_clean['sma_7'] = current_train_clean['y'].rolling(window=7, min_periods=1).mean()\n",
        "    current_train_clean['sma_30'] = current_train_clean['y'].rolling(window=30, min_periods=1).mean()\n",
        "\n",
        "    # Include momentum indicators\n",
        "    current_train_clean['momentum_5'] = current_train_clean['y'] - current_train_clean['y'].shift(5)\n",
        "    current_train_clean['momentum_10'] = current_train_clean['y'] - current_train_clean['y'].shift(10)\n",
        "\n",
        "    # Include volatility (rolling std)\n",
        "    current_train_clean['volatility_7'] = current_train_clean['y'].rolling(window=7, min_periods=1).std()\n",
        "    current_train_clean['volatility_30'] = current_train_clean['y'].rolling(window=30, min_periods=1).std()\n",
        "\n",
        "    # Include trend strength (price relative to moving average)\n",
        "    current_train_clean['trend_strength'] = current_train_clean['y'] - current_train_clean['sma_30']\n",
        "\n",
        "    # Drop rows with NaN\n",
        "    current_train_clean = current_train_clean.dropna()\n",
        "\n",
        "    if len(current_train_clean) < 50:\n",
        "        print(\"  Insufficient training data after feature engineering. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Initialize and Fit Model with Reduced Seasonality\n",
        "    model_2 = Prophet(\n",
        "        daily_seasonality=False,\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=True,\n",
        "        seasonality_mode='additive',\n",
        "        changepoint_prior_scale=0.05,  # Lower value is less flexible (prevents overfitting)\n",
        "        interval_width=0.95,\n",
        "        uncertainty_samples=1000,\n",
        "        seasonality_prior_scale=0.1,\n",
        "        holidays=holidays\n",
        "    )\n",
        "\n",
        "    # Add regressors to the model\n",
        "    regressors = ['lag1', 'lag2', 'lag3', 'lag7', 'sma_7', 'sma_30', 'momentum_5', 'momentum_10', 'volatility_7', 'volatility_30', 'trend_strength']\n",
        "\n",
        "    for reg in regressors:\n",
        "        model_2.add_regressor(reg)\n",
        "    model_2.add_seasonality(name='quarterly', period=91.3, fourier_order=5)\n",
        "\n",
        "    model_2.fit(current_train_clean)\n",
        "\n",
        "    # Forecasting with Actual Values (not recursive)\n",
        "    history_y = current_train_clean['y'].values.tolist()\n",
        "    fold_predictions = []\n",
        "    actual_dates = test_df['ds'].values\n",
        "\n",
        "    for step in range(len(test_df)):\n",
        "        # Create future dataframe\n",
        "        future_step_df = pd.DataFrame({'ds': [actual_dates[step]]})\n",
        "\n",
        "        # Calculate regressors from history\n",
        "        history_array = np.array(history_y)\n",
        "\n",
        "        future_step_df['lag1'] = history_array[-1]\n",
        "        future_step_df['lag2'] = history_array[-2]\n",
        "        future_step_df['lag3'] = history_array[-3]\n",
        "        future_step_df['lag7'] = history_array[-7] if len(history_array) >= 7 else history_array[0]\n",
        "\n",
        "        future_step_df['sma_7'] = history_array[-7:].mean() if len(history_array) >= 7 else history_array.mean()\n",
        "        future_step_df['sma_30'] = history_array[-30:].mean() if len(history_array) >= 30 else history_array.mean()\n",
        "\n",
        "        future_step_df['momentum_5'] = history_array[-1] - (history_array[-6] if len(history_array) >= 6 else history_array[0])\n",
        "        future_step_df['momentum_10'] = history_array[-1] - (history_array[-11] if len(history_array) >= 11 else history_array[0])\n",
        "\n",
        "        future_step_df['volatility_7'] = np.std(history_array[-7:]) if len(history_array) >= 7 else 0\n",
        "        future_step_df['volatility_30'] = np.std(history_array[-30:]) if len(history_array) >= 30 else 0\n",
        "\n",
        "        sma_30_current = history_array[-30:].mean() if len(history_array) >= 30 else history_array.mean()\n",
        "        future_step_df['trend_strength'] = history_array[-1] - sma_30_current\n",
        "\n",
        "        # Make prediction with intervals\n",
        "        forecast_step = model_2.predict(future_step_df)\n",
        "        fold_predictions.append(forecast_step)\n",
        "\n",
        "        # Use actual value for next iteration\n",
        "        actual_log_value = test_df.iloc[step]['y']\n",
        "        history_y.append(actual_log_value)\n",
        "\n",
        "    # Calculate Metrics with Coverage\n",
        "    fold_forecast_df = pd.concat(fold_predictions).reset_index(drop=True)\n",
        "    all_fold_forecasts.append(fold_forecast_df)\n",
        "\n",
        "    # Reverse log transformation\n",
        "    y_true_original_2 = np.expm1(test_df['y'].values)\n",
        "    y_pred_original_2 = np.expm1(fold_forecast_df['yhat'].values)\n",
        "    y_lower_original_2 = np.expm1(fold_forecast_df['yhat_lower'].values)\n",
        "    y_upper_original_2 = np.expm1(fold_forecast_df['yhat_upper'].values)\n",
        "\n",
        "    # Store predictions for this fold\n",
        "    for j in range(len(test_df)):\n",
        "        all_predictions_list.append({\n",
        "            'ds': test_df.iloc[j]['ds'],\n",
        "            'y_actual_log': test_df.iloc[j]['y'],\n",
        "            'y_actual': y_true_original_2[j],\n",
        "            'y_pred_log': fold_forecast_df.iloc[j]['yhat'],\n",
        "            'y_pred': y_pred_original_2[j],\n",
        "            'y_lower_log': fold_forecast_df.iloc[j]['yhat_lower'],\n",
        "            'y_lower': y_lower_original_2[j],\n",
        "            'y_upper_log': fold_forecast_df.iloc[j]['yhat_upper'],\n",
        "            'y_upper': y_upper_original_2[j],\n",
        "            'fold': i + 1\n",
        "        })\n",
        "\n",
        "    # Standard metrics\n",
        "    mae = mean_absolute_error(y_true_original_2, y_pred_original_2)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_original_2, y_pred_original_2))\n",
        "    mape = np.mean(np.abs((y_true_original_2 - y_pred_original_2) / (y_true_original_2 + 1e-8))) * 100\n",
        "    r2 = r2_score(y_true_original_2, y_pred_original_2)\n",
        "\n",
        "    # Coverage metrics\n",
        "    in_interval = np.sum((y_true_original_2 >= y_lower_original_2) & (y_true_original_2 <= y_upper_original_2))\n",
        "    coverage = (in_interval / len(y_true_original_2)) * 100\n",
        "    avg_interval_width = np.mean(y_upper_original_2 - y_lower_original_2)\n",
        "    avg_interval_width_pct = np.mean((y_upper_original_2 - y_lower_original_2) / y_true_original_2) * 100\n",
        "\n",
        "\n",
        "    metrics['mae'].append(mae)\n",
        "    metrics['rmse'].append(rmse)\n",
        "    metrics['mape'].append(mape)\n",
        "    metrics['r2'].append(r2)\n",
        "    coverage_metrics['in_coverage'].append(coverage)\n",
        "    coverage_metrics['coverage_width'].append(avg_interval_width_pct)\n",
        "\n",
        "    # Move the training window (candlestick method)\n",
        "    train_end += horizon\n",
        "\n",
        "    print(f\"    MAE: {mae:.2f} | RMSE: {rmse:.2f} | MAPE: {mape:.2f}% | R²: {r2:.3f}\")\n",
        "    print(f\"    Coverage: {coverage:.1f}% | Avg Width: {avg_interval_width_pct:.1f}%\")\n",
        "\n",
        "# Create comprehensive predictions dataframe\n",
        "predictions_df_with_regressors_nr = pd.DataFrame(all_predictions_list)\n",
        "\n",
        "# Aggregate and Display Final Metrics\n",
        "print(\"AGGREGATED METRICS (Walk-Forward Validation with added regressors)\")\n",
        "\n",
        "agg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "for k, v in agg_metrics.items():\n",
        "    std_val = np.std(metrics[k])\n",
        "    print(f\"{k.upper():8} = {v:7.3f} (±{std_val:.3f})\")\n",
        "\n",
        "print(f\"\\nCOVERAGE  = {np.mean(coverage_metrics['in_coverage']):7.1f}% (±{np.std(coverage_metrics['in_coverage']):.1f}%)\")\n",
        "print(f\"AVG WIDTH = {np.mean(coverage_metrics['coverage_width']):7.1f}% (±{np.std(coverage_metrics['coverage_width']):.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although this model produces decent results, this is not truly recursive in nature. Rather after each predction, it uses the original values of regressors, assuming each day the values up to the previous day will be fed to the model."
      ],
      "metadata": {
        "id": "NTYI2xjYSlDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now similar to the model before, I'll store and check the predictions of this model."
      ],
      "metadata": {
        "id": "vFEpYOdkS5Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving after running model with regressors\n",
        "import pickle\n",
        "predictions_df_with_regressors_nr.to_csv('predictions_with_regressors_nr.csv', index=False)\n",
        "with open('metrics_with_reg.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)\n",
        "print(\"Saved predictions_df_with_regressors_nr\")"
      ],
      "metadata": {
        "id": "Y3r5TwIyKAyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df_with_regressors_nr = pd.read_csv('predictions_with_regressors_nr.csv', parse_dates=['ds'])\n",
        "predictions_df_with_regressors_nr.head()"
      ],
      "metadata": {
        "id": "UP9EWAeUKyf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll train a true recursive model. Since the errors can compound on such models, I'll use a shorter horizon of 14 days and a higher changepoint prior scale (0.5)."
      ],
      "metadata": {
        "id": "IAVeO_KaTBwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for iterative traininng\n",
        "initial_train_size = int(0.9 * len(train_df_log))\n",
        "horizon = 14\n",
        "n_iterations = (len(train_df_log) - initial_train_size) // horizon\n",
        "# Storage for metrics\n",
        "metrics = {'mae': [], 'rmse': [], 'mape': [], 'r2': []}"
      ],
      "metadata": {
        "id": "m8Rx7E23ENwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Walk forward validation and performance metrics with autoregression (True Recursive)\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Main Walk-Forward Loop\n",
        "train_end = initial_train_size\n",
        "all_fold_forecasts = []\n",
        "coverage_metrics = {'in_coverage': [], 'coverage_width': []}\n",
        "\n",
        "# Store all predictions for later visualization\n",
        "all_predictions_list = []\n",
        "\n",
        "for i in range(n_iterations):\n",
        "    # Define current training and testing windows\n",
        "    current_train = train_df_log.iloc[:train_end].copy()\n",
        "    test_start = train_end\n",
        "    test_end = min(train_end + horizon, len(train_df_log))\n",
        "    test_df = train_df_log.iloc[test_start:test_end].copy()\n",
        "\n",
        "    if len(test_df) == 0:\n",
        "        break\n",
        "\n",
        "    print(f\"Iteration {i+1}/{n_iterations}: Training on {len(current_train)} data points, testing on {len(test_df)}.\")\n",
        "\n",
        "    # Recalculate features on the loop to avoid leakage\n",
        "    current_train_clean = current_train[['ds', 'y']].copy()\n",
        "\n",
        "    # Include basic lags\n",
        "    current_train_clean['lag1'] = current_train_clean['y'].shift(1)\n",
        "    current_train_clean['lag2'] = current_train_clean['y'].shift(2)\n",
        "    current_train_clean['lag3'] = current_train_clean['y'].shift(3)\n",
        "    current_train_clean['lag7'] = current_train_clean['y'].shift(7)\n",
        "\n",
        "    # Include moving averages\n",
        "    current_train_clean['sma_7'] = current_train_clean['y'].rolling(window=7, min_periods=1).mean()\n",
        "    current_train_clean['sma_30'] = current_train_clean['y'].rolling(window=30, min_periods=1).mean()\n",
        "\n",
        "    # Include momentum indicators\n",
        "    current_train_clean['momentum_5'] = current_train_clean['y'] - current_train_clean['y'].shift(5)\n",
        "    current_train_clean['momentum_10'] = current_train_clean['y'] - current_train_clean['y'].shift(10)\n",
        "\n",
        "    # Include volatility (rolling std)\n",
        "    current_train_clean['volatility_7'] = current_train_clean['y'].rolling(window=7, min_periods=1).std()\n",
        "    current_train_clean['volatility_30'] = current_train_clean['y'].rolling(window=30, min_periods=1).std()\n",
        "\n",
        "    # Include trend strength (price relative to moving average)\n",
        "    current_train_clean['trend_strength'] = current_train_clean['y'] - current_train_clean['sma_30']\n",
        "\n",
        "    # Drop rows with NaN\n",
        "    current_train_clean = current_train_clean.dropna()\n",
        "\n",
        "    if len(current_train_clean) < 50:\n",
        "        print(\"  Insufficient training data after feature engineering. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Initialize and Fit Model with Reduced Seasonality\n",
        "    model_2 = Prophet(\n",
        "        daily_seasonality=False,\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=True,\n",
        "        seasonality_mode='additive',\n",
        "        changepoint_prior_scale=0.5,  # Lower value is less flexible (prevents overfitting)\n",
        "        interval_width=0.95,\n",
        "        uncertainty_samples=1000,\n",
        "        seasonality_prior_scale=0.1,\n",
        "        holidays=holidays\n",
        "    )\n",
        "\n",
        "    # Add regressors to the model\n",
        "    regressors = ['lag1', 'lag2', 'lag3', 'lag7', 'sma_7', 'sma_30', 'momentum_5', 'momentum_10', 'volatility_7', 'volatility_30', 'trend_strength']\n",
        "\n",
        "    for reg in regressors:\n",
        "        model_2.add_regressor(reg)\n",
        "    model_2.add_seasonality(name='quarterly', period=91.3, fourier_order=5)\n",
        "\n",
        "    model_2.fit(current_train_clean)\n",
        "\n",
        "    # TRUE RECURSIVE Forecasting (using predicted values)\n",
        "    history_y = current_train_clean['y'].values.tolist()\n",
        "    fold_predictions = []\n",
        "    actual_dates = test_df['ds'].values\n",
        "\n",
        "    for step in range(len(test_df)):\n",
        "        # Create future dataframe\n",
        "        future_step_df = pd.DataFrame({'ds': [actual_dates[step]]})\n",
        "\n",
        "        # Calculate regressors from history\n",
        "        history_array = np.array(history_y)\n",
        "\n",
        "        future_step_df['lag1'] = history_array[-1]\n",
        "        future_step_df['lag2'] = history_array[-2]\n",
        "        future_step_df['lag3'] = history_array[-3]\n",
        "        future_step_df['lag7'] = history_array[-7] if len(history_array) >= 7 else history_array[0]\n",
        "\n",
        "        future_step_df['sma_7'] = history_array[-7:].mean() if len(history_array) >= 7 else history_array.mean()\n",
        "        future_step_df['sma_30'] = history_array[-30:].mean() if len(history_array) >= 30 else history_array.mean()\n",
        "\n",
        "        future_step_df['momentum_5'] = history_array[-1] - (history_array[-6] if len(history_array) >= 6 else history_array[0])\n",
        "        future_step_df['momentum_10'] = history_array[-1] - (history_array[-11] if len(history_array) >= 11 else history_array[0])\n",
        "\n",
        "        future_step_df['volatility_7'] = np.std(history_array[-7:]) if len(history_array) >= 7 else 0\n",
        "        future_step_df['volatility_30'] = np.std(history_array[-30:]) if len(history_array) >= 30 else 0\n",
        "\n",
        "        sma_30_current = history_array[-30:].mean() if len(history_array) >= 30 else history_array.mean()\n",
        "        future_step_df['trend_strength'] = history_array[-1] - sma_30_current\n",
        "\n",
        "        # Make prediction with intervals\n",
        "        forecast_step = model_2.predict(future_step_df)\n",
        "        fold_predictions.append(forecast_step)\n",
        "\n",
        "        # TRUE RECURSIVE: Use PREDICTED value for next iteration\n",
        "        predicted_log_value = forecast_step['yhat'].values[0]\n",
        "        history_y.append(predicted_log_value)\n",
        "\n",
        "    # Calculate Metrics with Coverage\n",
        "    fold_forecast_df = pd.concat(fold_predictions).reset_index(drop=True)\n",
        "    all_fold_forecasts.append(fold_forecast_df)\n",
        "\n",
        "    # Reverse log transformation\n",
        "    y_true_original_2 = np.expm1(test_df['y'].values)\n",
        "    y_pred_original_2 = np.expm1(fold_forecast_df['yhat'].values)\n",
        "    y_lower_original_2 = np.expm1(fold_forecast_df['yhat_lower'].values)\n",
        "    y_upper_original_2 = np.expm1(fold_forecast_df['yhat_upper'].values)\n",
        "\n",
        "    # Store predictions for this fold\n",
        "    for j in range(len(test_df)):\n",
        "        all_predictions_list.append({\n",
        "            'ds': test_df.iloc[j]['ds'],\n",
        "            'y_actual_log': test_df.iloc[j]['y'],\n",
        "            'y_actual': y_true_original_2[j],\n",
        "            'y_pred_log': fold_forecast_df.iloc[j]['yhat'],\n",
        "            'y_pred': y_pred_original_2[j],\n",
        "            'y_lower_log': fold_forecast_df.iloc[j]['yhat_lower'],\n",
        "            'y_lower': y_lower_original_2[j],\n",
        "            'y_upper_log': fold_forecast_df.iloc[j]['yhat_upper'],\n",
        "            'y_upper': y_upper_original_2[j],\n",
        "            'fold': i + 1\n",
        "        })\n",
        "\n",
        "    # Standard metrics\n",
        "    mae = mean_absolute_error(y_true_original_2, y_pred_original_2)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_original_2, y_pred_original_2))\n",
        "    mape = np.mean(np.abs((y_true_original_2 - y_pred_original_2) / (y_true_original_2 + 1e-8))) * 100\n",
        "    r2 = r2_score(y_true_original_2, y_pred_original_2)\n",
        "\n",
        "    # Coverage metrics\n",
        "    in_interval = np.sum((y_true_original_2 >= y_lower_original_2) & (y_true_original_2 <= y_upper_original_2))\n",
        "    coverage = (in_interval / len(y_true_original_2)) * 100\n",
        "    avg_interval_width = np.mean(y_upper_original_2 - y_lower_original_2)\n",
        "    avg_interval_width_pct = np.mean((y_upper_original_2 - y_lower_original_2) / y_true_original_2) * 100\n",
        "\n",
        "    metrics['mae'].append(mae)\n",
        "    metrics['rmse'].append(rmse)\n",
        "    metrics['mape'].append(mape)\n",
        "    metrics['r2'].append(r2)\n",
        "    coverage_metrics['in_coverage'].append(coverage)\n",
        "    coverage_metrics['coverage_width'].append(avg_interval_width_pct)\n",
        "\n",
        "    # Move the training window (candlestick method)\n",
        "    train_end += horizon\n",
        "\n",
        "    print(f\"    MAE: {mae:.2f} | RMSE: {rmse:.2f} | MAPE: {mape:.2f}% | R²: {r2:.3f}\")\n",
        "    print(f\"    Coverage: {coverage:.1f}% | Avg Width: {avg_interval_width_pct:.1f}%\")\n",
        "\n",
        "# Create comprehensive predictions dataframe\n",
        "predictions_df_with_regressors = pd.DataFrame(all_predictions_list)\n",
        "\n",
        "# Aggregate and Display Final Metrics\n",
        "print(\"AGGREGATED METRICS (Walk-Forward Validation with TRUE RECURSIVE)\")\n",
        "\n",
        "agg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "for k, v in agg_metrics.items():\n",
        "    std_val = np.std(metrics[k])\n",
        "    print(f\"{k.upper():8} = {v:7.3f} (±{std_val:.3f})\")\n",
        "\n",
        "print(f\"\\nCOVERAGE  = {np.mean(coverage_metrics['in_coverage']):7.1f}% (±{np.std(coverage_metrics['in_coverage']):.1f}%)\")\n",
        "print(f\"AVG WIDTH = {np.mean(coverage_metrics['coverage_width']):7.1f}% (±{np.std(coverage_metrics['coverage_width']):.1f}%)\")"
      ],
      "metadata": {
        "id": "9_m14LK0BGSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although this has improved MAE, RMSE, and MAPE, the r2 and coverage values have worsened. Now I'll save these results as well."
      ],
      "metadata": {
        "id": "aApwb4J-EE-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions after running model with regressors\n",
        "import pickle\n",
        "predictions_df_with_regressors.to_csv('predictions_with_regressors.csv', index=False)\n",
        "with open('metrics_with_reg.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)\n",
        "print(\"Saved predictions_df_with_regressors\")"
      ],
      "metadata": {
        "id": "8zlU6o4vXb7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load predictions to check\n",
        "predictions_df_with_regressors = pd.read_csv('predictions_with_regressors.csv', parse_dates=['ds'])\n",
        "predictions_df_with_regressors.head()"
      ],
      "metadata": {
        "id": "aqedDQYtlZeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeamldBeqY3M"
      },
      "source": [
        "Now I'll compare all my model's metrices with a persistence model. I'll use the same training size and horizon (60 days) as my previous models to maintain comparability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5wWTWLwwDkY"
      },
      "outputs": [],
      "source": [
        "# Testing model's validation with persistance model (walk-forward)\n",
        "train_end = initial_train_size\n",
        "persistence_metrics = {'mae': [], 'rmse': [], 'mape': [], 'r2': []}\n",
        "\n",
        "# Store all predictions\n",
        "all_predictions_list_persistence = []\n",
        "\n",
        "for i in range(n_iterations):\n",
        "    # Define train and test windows\n",
        "    test_start = train_end\n",
        "    test_end = min(train_end + horizon, len(train_df_log))\n",
        "    test_df = train_df_log.iloc[test_start:test_end].copy()\n",
        "\n",
        "    if len(test_df) == 0:\n",
        "        break\n",
        "\n",
        "    # Intoducing persistence prediction: tomorrow's price = today's price\n",
        "    y_pred_log = test_df['y'].shift(1).fillna(test_df['y'].iloc[0])\n",
        "\n",
        "    # Transform back to original scale\n",
        "    y_true_pers = np.expm1(test_df['y'].values)\n",
        "    y_pred_pers = np.expm1(y_pred_log.values)\n",
        "\n",
        "    # Store predictions for this fold\n",
        "    for j in range(len(test_df)):\n",
        "        all_predictions_list_persistence.append({\n",
        "            'ds': test_df.iloc[j]['ds'],\n",
        "            'y_actual_log': test_df.iloc[j]['y'],\n",
        "            'y_actual': y_true_pers[j],\n",
        "            'y_pred_log': y_pred_log.iloc[j],\n",
        "            'y_pred': y_pred_pers[j],\n",
        "            'fold': i + 1\n",
        "        })\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_true_pers, y_pred_pers)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_pers, y_pred_pers))\n",
        "    mape = np.mean(np.abs((y_true_pers - y_pred_pers) / (y_true_pers + 1e-8))) * 100\n",
        "    r2 = r2_score(y_true_pers, y_pred_pers)\n",
        "\n",
        "    # Store metrics\n",
        "    persistence_metrics['mae'].append(mae)\n",
        "    persistence_metrics['rmse'].append(rmse)\n",
        "    persistence_metrics['mape'].append(mape)\n",
        "    persistence_metrics['r2'].append(r2)\n",
        "\n",
        "    # Move window forward (candlestick method)\n",
        "    train_end += horizon\n",
        "\n",
        "# Create predictions dataframe\n",
        "predictions_df_persistence = pd.DataFrame(all_predictions_list_persistence)\n",
        "\n",
        "# Aggregated Results\n",
        "print(f\"\\nEvaluated on {n_iterations} folds with {horizon}-day horizon\\n\")\n",
        "\n",
        "print(\"PERSISTENCE MODEL RESULTS\")\n",
        "\n",
        "for metric_name, values in persistence_metrics.items():\n",
        "    mean_val = np.mean(values)\n",
        "    std_val = np.std(values)\n",
        "    print(f\"{metric_name.upper():8} = {mean_val:7.3f} (±{std_val:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll save the results of the persistence model as well for future comparison. The model results in slightly better but comparable metrics compared to the model with regressors (Model 2 with autoregression, which is not true recursive)."
      ],
      "metadata": {
        "id": "vH1NoebWPAKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77AdJbDvwU_6"
      },
      "outputs": [],
      "source": [
        "# Save predictions after persistence model\n",
        "predictions_df_persistence.to_csv('predictions_persistence.csv', index=False)\n",
        "print(\"Saved predictions_df_persistence\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load predictions to check values\n",
        "predictions_df_persistence = pd.read_csv('predictions_persistence.csv', parse_dates=['ds'])\n",
        "predictions_df_persistence.head()"
      ],
      "metadata": {
        "id": "mRpYH3m3mcu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYymVmKMwWZN"
      },
      "source": [
        "Now I'll create a table with side by side precitions of 4 models along with the actual close value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare model 1 (cross validation) predictions\n",
        "cv_predictions = predictions_cv_no_regressors[['ds', 'y_actual', 'yhat_actual']].copy()\n",
        "cv_predictions = cv_predictions.rename(columns={'yhat_actual': 'y_pred_no_reg'})\n",
        "\n",
        "# Start with the model without regressors\n",
        "comparison_all_models = cv_predictions.copy()\n",
        "\n",
        "# Merge with the model with regressors\n",
        "comparison_all_models = comparison_all_models.merge(predictions_df_with_regressors_nr[['ds', 'y_pred']], on='ds', how='left')\n",
        "comparison_all_models = comparison_all_models.rename(columns={'y_pred': 'y_pred_with_reg_nr'})\n",
        "\n",
        "# Merge with the true recursive model with regressors\n",
        "comparison_all_models = comparison_all_models.merge(predictions_df_with_regressors[['ds', 'y_pred']], on='ds', how='left')\n",
        "comparison_all_models = comparison_all_models.rename(columns={'y_pred': 'y_pred_with_reg'})\n",
        "\n",
        "# Merge with persistence model\n",
        "comparison_all_models = comparison_all_models.merge(predictions_df_persistence[['ds', 'y_pred']], on='ds', how='left')\n",
        "comparison_all_models = comparison_all_models.rename(columns={'y_pred': 'y_pred_persistence'})\n",
        "\n",
        "# Reorder columns\n",
        "comparison_all_models = comparison_all_models[['ds', 'y_actual', 'y_pred_no_reg',\n",
        "                                                 'y_pred_with_reg_nr', 'y_pred_with_reg', 'y_pred_persistence']]\n",
        "\n",
        "# Count missing values\n",
        "print(\"MISSING VALUES CHECK\")\n",
        "print(f\"No Regressors (CV):      {comparison_all_models['y_pred_no_reg'].isna().sum()} missing\")\n",
        "print(f\"With Regressors (NR):    {comparison_all_models['y_pred_with_reg_nr'].isna().sum()} missing\")\n",
        "print(f\"With Regressors (True Recursive):   {comparison_all_models['y_pred_with_reg'].isna().sum()} missing\")\n",
        "print(f\"Persistence (WFV):       {comparison_all_models['y_pred_persistence'].isna().sum()} missing\")\n",
        "\n",
        "# Keep only rows where ALL three models have predictions\n",
        "comparison_all_models_complete = comparison_all_models.dropna(subset=['y_pred_no_reg', 'y_pred_with_reg_nr', 'y_pred_with_reg', 'y_pred_persistence'])\n",
        "\n",
        "comparison_all_models_complete.head()"
      ],
      "metadata": {
        "id": "4kzUUFcnprCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll visualize the predictions of all 4 models to see how each model is actually performing. I'll plot the predictions of all these models besides the original y value (Adjusted Close Price)."
      ],
      "metadata": {
        "id": "uMACiKa2TmMA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGSjST7isZhY"
      },
      "outputs": [],
      "source": [
        "# visualize all 3 models' predictions side by side\n",
        "plt.figure(figsize=(20, 10))\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.plot(comparison_all_models_complete['ds'], comparison_all_models_complete['y_actual'], label='Actual', color = 'yellow', alpha = 0.6)\n",
        "plt.plot(comparison_all_models_complete['ds'], comparison_all_models_complete['y_pred_no_reg'], label='Model 1 (without regressors)', color = 'orange', alpha=0.6)\n",
        "plt.plot(comparison_all_models_complete['ds'], comparison_all_models_complete['y_pred_with_reg_nr'], label='Model 2 (with regressors)', color = 'red', alpha=0.6)\n",
        "plt.plot(comparison_all_models_complete['ds'], comparison_all_models_complete['y_pred_with_reg'], label='Model 2 (true recursive)', color = 'blue', alpha=0.6)\n",
        "plt.plot(comparison_all_models_complete['ds'], comparison_all_models_complete['y_pred_persistence'], label='Persistence', color = 'green', alpha=0.6)\n",
        "plt.xticks(rotation=45) # Rotate x-axis labels for better readability\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Comparison of 3 Models')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the direct link to the dataset. The dataset is Adobe stock data from the datacard at this address https://www.kaggle.com/datasets/paultimothymooney/stock-market-data/data."
      ],
      "metadata": {
        "id": "BU0z1qe0WVcb"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}